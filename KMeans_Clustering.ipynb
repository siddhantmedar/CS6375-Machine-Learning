{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KMeans Clustering.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3y1XaoQK9i_",
        "outputId": "1c1c96fe-9d2f-4e9d-b1d2-b9a195dc4eb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Centroids After Iteration  1\n",
            "[1552, 1993, 381, 875, 309]\n",
            "Cluster Number : Number of Tweets\n",
            "1 : 313\n",
            "2 : 516\n",
            "3 : 374\n",
            "4 : 741\n",
            "5 : 56\n",
            "Sum of squared Error (SSE) after iteration no. 1\n",
            "732.0910175899891\n",
            "Updated Centroids After Iteration  2\n",
            "[1552, 1993, 381, 1453, 698]\n",
            "Cluster Number : Number of Tweets\n",
            "1 : 428\n",
            "2 : 420\n",
            "3 : 915\n",
            "4 : 151\n",
            "5 : 86\n",
            "Sum of squared Error (SSE) after iteration no. 2\n",
            "727.6378303499828\n",
            "Updated Centroids After Iteration  3\n",
            "[1552, 1993, 381, 1453, 1259]\n",
            "Cluster Number : Number of Tweets\n",
            "1 : 309\n",
            "2 : 346\n",
            "3 : 132\n",
            "4 : 826\n",
            "5 : 387\n",
            "Sum of squared Error (SSE) after iteration no. 3\n",
            "707.7330352699837\n",
            "Updated Centroids After Iteration  4\n",
            "[1552, 1993, 381, 1453, 1259]\n",
            "KMeans Clustering converged after iteration no. = 4 for K = 5\n"
          ]
        }
      ],
      "source": [
        "# Tweets KMeans Clustering by:\n",
        "# Siddhant Medar (SSM200002)\n",
        "# Adithya Iyer (ASI200000)\n",
        "\n",
        "import math\n",
        "import random\n",
        "import re\n",
        "import copy\n",
        "import urllib\n",
        "import urllib.request\n",
        "\n",
        "\n",
        "# Data Preprocessing\n",
        "def preprocess(p):\n",
        "    twt = []\n",
        "    f = urllib.request.urlopen(p)\n",
        "\n",
        "    lines = [x.decode(\"cp1252\").strip() for x in f]\n",
        "    for ln in lines:\n",
        "        tw = ln.split('|')[2].split('http://')[0]\n",
        "        tw = re.sub('@|#', '', tw)\n",
        "        tw = tw.lower()\n",
        "        twt.append(tw)\n",
        "\n",
        "    return twt\n",
        "\n",
        "\n",
        "# Calculating Jaccard Distance\n",
        "def distance(x, y):\n",
        "    intersection = list(set(x) & set(y))\n",
        "    intrs = len(intersection)\n",
        "    Union = list(set(x) | set(y))\n",
        "    un_ln = len(Union)\n",
        "    overlap = (float(intrs) / un_ln)\n",
        "    return round(1 - overlap, 4)\n",
        "\n",
        "\n",
        "# output in the form of cluster no. and the number of tweets in that cluster.\n",
        "def clusters_output(cluster_no, k, twt_id):\n",
        "    cluster_id = []\n",
        "    print(\"Cluster Number : Number of Tweets\")\n",
        "    for i in range(k):\n",
        "        cluster_id.append([j for j, c in enumerate(cluster_no) if c == i])\n",
        "        tw_no = [y for y in cluster_id[i]]\n",
        "        print(i + 1, \":\", len([twt_id[y] for y in tw_no]))\n",
        "\n",
        "\n",
        "# computing the sum of squared errors\n",
        "def SSE(centroids, tweet_text, k):\n",
        "    sum_sq_error = 0\n",
        "    for i in range(k):\n",
        "        for j in range(len(tweet_text)):\n",
        "            sum_sq_error = sum_sq_error + math.pow(distance(tweet_text[j], centroids[i]), 2)\n",
        "    print(sum_sq_error)\n",
        "\n",
        "\n",
        "# updating the centroids at every iteration\n",
        "def update_centroid(twt_id, cluster, tweet_text, k):\n",
        "    ind = []\n",
        "    upd_centroid_tweet_id = []\n",
        "\n",
        "    for i in range(k):\n",
        "        ind.append([j for j, cls in enumerate(cluster) if cls == i])\n",
        "        cl = ind[i]\n",
        "\n",
        "        # cl gives the indices of the elements of every cluster k\n",
        "\n",
        "        if len(cl) != 0:\n",
        "            tweet = [tweet_text[s] for s in cl]\n",
        "            similarity_distance = [[distance(tweet[i], tweet[j]) for j in range(len(cl))] for i in range(len(cl))]\n",
        "\n",
        "            total_similarity = [sum(i) for i in similarity_distance]\n",
        "            # idx of the point closer to all the other points\n",
        "            upd_centroid_tweet_id.append(cl[(total_similarity.index(min(total_similarity)))])\n",
        "    updated_twt_centroid = [twt_id[x] for x in upd_centroid_tweet_id]\n",
        "    return updated_twt_centroid\n",
        "\n",
        "\n",
        "# k-means clustering implementation from scratch\n",
        "def kmeans(twt_id, centroids, twt_txt, ln, k):\n",
        "    count = 0\n",
        "    for itr in range(50):\n",
        "        count = count + 1\n",
        "        cluster_id = []\n",
        "        cnt = 0\n",
        "        for i in range(ln):\n",
        "            cnt += 1\n",
        "            dist = [distance(twt_txt[i].split(' '), centroids[j].split(' ')) for j in range(k)]\n",
        "            dist_id = dist.index(min(dist))\n",
        "            cluster_id.append(dist_id)\n",
        "\n",
        "        nw_centroid = update_centroid(twt_id, cluster_id, twt_txt, k)\n",
        "        print(\"Updated Centroids After Iteration \", itr + 1)\n",
        "        print(nw_centroid)\n",
        "        cen = [twt_txt.index(item) for item in centroids]\n",
        "        if cen == nw_centroid:\n",
        "            print(\"KMeans Clustering converged after iteration no. = {} for K = {}\".format(itr + 1, k))\n",
        "            break\n",
        "        centroids_id = copy.deepcopy(nw_centroid)\n",
        "        centroids = []\n",
        "        for twt_idx in centroids_id:\n",
        "            centroids.append(twt_txt[twt_idx])\n",
        "\n",
        "        clusters_output(cluster_id, k, twt_id)\n",
        "        print(\"Sum of squared Error (SSE) after iteration no.\", itr + 1)\n",
        "        SSE(centroids, twt_txt, k)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    path = \"https://raw.githubusercontent.com/siddhantmedar/CS6375-Machine-Learning/main/Tweets/foxnewshealth.txt\"\n",
        "    # path = \"https://raw.githubusercontent.com/siddhantmedar/CS6375-Machine-Learning/main/Tweets/msnhealthnews.txt\"\n",
        "    # path = \"https://github.com/siddhantmedar/CS6375-Machine-Learning/blob/main/Tweets/usnewshealth.txt\"\n",
        "\n",
        "    # use k as 5 (Tested for several K values)\n",
        "    k = 5\n",
        "    # preprocess the tweets\n",
        "    tweets = preprocess(path)\n",
        "    length = len(tweets)\n",
        "    twt_ids = []\n",
        "    for i in range(len(tweets)):\n",
        "        twt_ids.append(i)\n",
        "    tweets_centroid = random.sample(tweets, k)\n",
        "\n",
        "    # run k-means clustering on the tweets\n",
        "    kmeans(twt_ids, tweets_centroid, tweets, length, k)\n"
      ]
    }
  ]
}